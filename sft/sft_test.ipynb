{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac4097f",
   "metadata": {},
   "source": [
    "## Test Data ë¡œ ì •í™•ë„ / Confusion Matrix í™•ì¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f371d2",
   "metadata": {},
   "source": [
    "### LoRA STF model test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e0ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "386a03be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_finetuned_model(model_path, device=\"cuda\"):\n",
    "    \"\"\"íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    logger.info(f\"Loading fine-tuned model from {model_path}\")\n",
    "    \n",
    "    # í”„ë¡œì„¸ì„œì™€ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    # ëª¨ë¸ ë¡œë“œ (LoRA ì–´ëŒ‘í„° í¬í•¨)\n",
    "    base_model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        trust_remote_code=True,\n",
    "        revision=\"main\",\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # LoRA ì–´ëŒ‘í„° ë¡œë“œ\n",
    "    model = PeftModel.from_pretrained(model, model_path)\n",
    "    model = model.merge_and_unload()  # LoRAë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ë³‘í•©\n",
    "    model.eval()\n",
    "    \n",
    "    return model, processor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4decd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model(device=\"cuda\"):\n",
    "    \"\"\"íŒŒì¸íŠœë‹ë˜ì§€ ì•Šì€ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    model_name = \"naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B\"\n",
    "    revision = \"v0.1.0\"  # ì›í•˜ëŠ” revisionìœ¼ë¡œ ê³ ì •\n",
    "\n",
    "    logger.info(f\"Loading base model: {model_name} ({revision})\")\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True, revision=revision)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, revision=revision)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        revision=revision,\n",
    "        device_map={\"\": device},\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    return model, processor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42986eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_device(inputs, device):\n",
    "    for k, v in inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            inputs[k] = v.to(device)\n",
    "        elif isinstance(v, list):\n",
    "            if all(isinstance(i, torch.Tensor) for i in v):\n",
    "                inputs[k] = [i.to(device) for i in v]\n",
    "        elif isinstance(v, dict):\n",
    "            for sub_k, sub_v in v.items():\n",
    "                if isinstance(sub_v, torch.Tensor):\n",
    "                    v[sub_k] = sub_v.to(device)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "592fe611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, processor, tokenizer, test_data_path, output_dir, model_path, device=\"cuda\"):\n",
    "    \"\"\"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í‰ê°€\"\"\"\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    logger.info(f\"Loading test data from {test_data_path}\")\n",
    "    with open(test_data_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    # ê°ì • ë¼ë²¨ ì •ì˜ (system promptì— ì •ì˜ëœ ìˆœì„œëŒ€ë¡œ)\n",
    "    emotions = ['í–‰ë³µ', 'ìŠ¬í””', 'ë¶„ë…¸', 'ê³µí¬', 'ë†€ëŒ', 'ì¤‘ë¦½']\n",
    "    \n",
    "    # ìƒì„¸ ë¡œê·¸ íŒŒì¼ ìƒì„±\n",
    "    log_filename = f\"{output_dir}/detailed_predictions_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "    log_file = open(log_filename, 'w', encoding='utf-8')\n",
    "    log_file.write(\"=== Emotion Classification Detailed Test Log ===\\n\")\n",
    "    log_file.write(f\"Test started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Model path: {model_path}\\n\")\n",
    "    log_file.write(f\"Total test samples: {len(test_data)}\\n\")\n",
    "    log_file.write(\"=\"*60 + \"\\n\\n\")\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    correct_count = 0\n",
    "    \n",
    "    logger.info(\"Starting evaluation...\")\n",
    "    for idx, sample in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
    "        # ë°ì´í„° ì¶”ì¶œ\n",
    "        image_path = None\n",
    "        user_text = \"\"\n",
    "        true_emotion = \"\"\n",
    "\n",
    "        for msg in sample['chat']:\n",
    "            content = msg['content']\n",
    "            if isinstance(content, dict):\n",
    "                if content.get(\"type\") == \"image\":\n",
    "                    image_path = content.get(\"image\")\n",
    "                elif content.get(\"type\") == \"text\" and msg[\"role\"] == \"user\":\n",
    "                    user_text = content.get(\"text\")\n",
    "                elif content.get(\"type\") == \"text\" and msg[\"role\"] == \"assistant\":\n",
    "                    true_emotion = content.get(\"text\")\n",
    "\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        conversations = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ì œì‹œëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ì¸ë¬¼ì˜ ê°ì •ì„ íŒë‹¨í•˜ì‹­ì‹œì˜¤. ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ ì—¬ì„¯ ê°€ì§€ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤ : í–‰ë³µ, ìŠ¬í””, ë¶„ë…¸, ê³µí¬, ë†€ëŒ, ì¤‘ë¦½.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image_path},  # âœ… ë°˜ë“œì‹œ í¬í•¨!\n",
    "                    {\"type\": \"text\", \"text\": user_text}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # 1. í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        text = processor.apply_chat_template(\n",
    "            conversations,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # 2. ì´ë¯¸ì§€ ë¡œë“œ\n",
    "        if not image_path or not os.path.exists(image_path):\n",
    "            print(f\"[{idx+1}] âš ï¸ ì´ë¯¸ì§€ ê²½ë¡œ ì—†ìŒ ë˜ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {image_path}\")\n",
    "            log_file.write(f\"[Sample {idx+1}] WARNING: Missing or invalid image path: {image_path}\\n\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                img.verify()  # ì´ë¯¸ì§€ í¬ë§·/í—¤ë” ìœ íš¨ì„±ë§Œ ê²€ì‚¬ (ë°ì´í„°ëŠ” ì½ì§€ ì•ŠìŒ)\n",
    "\n",
    "            with Image.open(image_path) as img:\n",
    "                img.load()  # ì‹¤ì œ ë°ì´í„° ë¡œë“œ (ê¹¨ì§„ ì´ë¯¸ì§€ í™•ì¸ìš©)\n",
    "                image = img.convert(\"RGB\").copy()  # ë³µì‚¬í•´ë‘ê³  íŒŒì¼ í•¸ë“¤ ë‹«ê¸°\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1}] âŒ ì´ë¯¸ì§€ ë¡œë”© ì˜¤ë¥˜: {e}\")\n",
    "            log_file.write(f\"[Sample {idx+1}] ERROR during image load: {e}\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        # print(f\"[{idx+1}] ğŸ“· image type: {type(image)} | size: {image.size}\")\n",
    "        \n",
    "        # 3. í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ í†µí•© ì „ì²˜ë¦¬ (ê¶Œì¥ ë°©ì‹)\n",
    "        try:\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=[image],\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "\n",
    "            # âœ… pixel_values ë³´ì™„\n",
    "            if \"pixel_values_images\" in inputs:\n",
    "                try:\n",
    "                    pixel_tensor = inputs[\"pixel_values_images\"][0][0]\n",
    "                    inputs[\"pixel_values\"] = pixel_tensor.to(device)\n",
    "\n",
    "                    # âœ… ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ í…ì„œë„ ì´ë™\n",
    "                    inputs[\"pixel_values_images\"] = [\n",
    "                        [t.to(device) if isinstance(t, torch.Tensor) else t for t in sublist]\n",
    "                        for sublist in inputs[\"pixel_values_images\"]\n",
    "                    ]\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"âŒ 'pixel_values_images' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "            else:\n",
    "                raise KeyError(\"âŒ 'pixel_values_images'ê°€ processor ê²°ê³¼ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "            # âœ… ë‚˜ë¨¸ì§€ í…ì„œ to(device)\n",
    "            inputs = {\n",
    "                k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                for k, v in inputs.items()\n",
    "            }\n",
    "            \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1}] âŒ processor ì˜¤ë¥˜: {e}\")\n",
    "            log_file.write(f\"[Sample {idx+1}] ERROR during processor call: {e}\\n\\n\")\n",
    "            continue\n",
    "        \n",
    "        # 4. ì¶”ë¡ \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=10,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                    # temperature=0.1,\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"[{idx+1}] âŒ generate ì˜¤ë¥˜: {e}\")\n",
    "            log_file.write(f\"[{idx+1}] ERROR: Generate failed: {e}\\n\\n\")\n",
    "            continue\n",
    "        \n",
    "        # ê²°ê³¼ ë””ì½”ë”© (generated_text ì¤‘ ê°ì • ë¼ë²¨ë§Œ ì¶”ì¶œ)\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # ê°ì • ë¼ë²¨ë§Œ ì •ì œí•´ì„œ ì¶”ì¶œ\n",
    "        emotion_labels = [\"í–‰ë³µ\", \"ìŠ¬í””\", \"ë¶„ë…¸\", \"ê³µí¬\", \"ë†€ëŒ\", \"ì¤‘ë¦½\"]\n",
    "        pattern = \"|\".join(emotion_labels)\n",
    "        match = re.search(pattern, generated_text)\n",
    "\n",
    "        predicted_emotion = match.group(0) if match else \"unknown\"\n",
    "        \n",
    "        # ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬\n",
    "        predictions.append(predicted_emotion)\n",
    "        true_labels.append(true_emotion)\n",
    "        is_correct = predicted_emotion == true_emotion\n",
    "        if is_correct:\n",
    "            correct_count += 1\n",
    "        \n",
    "        log_file.write(f\"[{idx+1}]\\nImage: {os.path.basename(image_path)}\\n\")\n",
    "        log_file.write(f\"Text: {user_text}\\nTrue: {true_emotion} | Predicted: {predicted_emotion} | {'âœ“' if is_correct else 'âœ—'}\\n\")\n",
    "        log_file.write(f\"Generated: {generated_text}\\n{'-'*40}\\n\\n\")\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            acc = correct_count / (idx + 1)\n",
    "            logger.info(f\"âœ… Progress: {idx+1}/{len(test_data)} | Accuracy: {acc:.4f}\")\n",
    "\n",
    "    final_accuracy = correct_count / len(predictions)\n",
    "    log_file.write(\"=\"*60 + \"\\n\")\n",
    "    log_file.write(f\"âœ… Final accuracy: {final_accuracy:.4f} ({correct_count}/{len(predictions)})\\n\")\n",
    "    log_file.write(f\"ğŸ•’ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.close()\n",
    "    \n",
    "    logger.info(f\"Detailed log saved to: {log_filename}\")\n",
    "    \n",
    "    return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f89e9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(true_labels, predictions):\n",
    "    \"\"\"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    \n",
    "    # ê¸°ë³¸ ë©”íŠ¸ë¦­\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    \n",
    "    # í´ë˜ìŠ¤ë³„ ë©”íŠ¸ë¦­\n",
    "    labels = sorted(set(true_labels + predictions))\n",
    "    class_report = classification_report(\n",
    "        true_labels, \n",
    "        predictions, \n",
    "        target_names=labels,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'class_report': class_report\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c3059d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, predictions, save_path=\"confusion_matrix.png\"):\n",
    "    \"\"\"í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\"\"\"\n",
    "    \n",
    "    # ë¼ë²¨ ì •ë ¬\n",
    "    labels = sorted(set(true_labels + predictions))\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬ ê³„ì‚°\n",
    "    cm = confusion_matrix(true_labels, predictions, labels=labels)\n",
    "    \n",
    "    # í•œê¸€ í°íŠ¸ ì„¤ì • (í•„ìš”ì‹œ)\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "    \n",
    "    # ì‹œê°í™”\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels\n",
    "    )\n",
    "    plt.title('Confusion Matrix - Emotion Classification', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_normalized = np.nan_to_num(cm_normalized)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm_normalized, \n",
    "        annot=True, \n",
    "        fmt='.2f', \n",
    "        cmap='Blues',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels\n",
    "    )\n",
    "    plt.title('Normalized Confusion Matrix - Emotion Classification', fontsize=16)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path.replace('.png', '_normalized.png'), dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a3689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(predictions, true_labels, metrics, output_dir):\n",
    "    \"\"\"ê²°ê³¼ ì €ì¥\"\"\"\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "    results_df = pd.DataFrame({\n",
    "        'true_label': true_labels,\n",
    "        'predicted_label': predictions,\n",
    "        'correct': [t == p for t, p in zip(true_labels, predictions)]\n",
    "    })\n",
    "    results_df.to_csv(f\"{output_dir}/predictions.csv\", index=False, encoding='utf-8')\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ì €ì¥\n",
    "    with open(f\"{output_dir}/metrics.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    report = f\"\"\"\n",
    "=== Emotion Classification Test Results ===\n",
    "\n",
    "Overall Metrics:\n",
    "- Accuracy: {metrics['accuracy']:.4f}\n",
    "- Precision: {metrics['precision']:.4f}\n",
    "- Recall: {metrics['recall']:.4f}\n",
    "- F1 Score: {metrics['f1_score']:.4f}\n",
    "\n",
    "Per-Class Performance:\n",
    "\"\"\"\n",
    "    \n",
    "    for emotion, scores in metrics['class_report'].items():\n",
    "        if emotion not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            report += f\"\\n{emotion}:\"\n",
    "            report += f\"\\n  - Precision: {scores['precision']:.4f}\"\n",
    "            report += f\"\\n  - Recall: {scores['recall']:.4f}\"\n",
    "            report += f\"\\n  - F1-Score: {scores['f1-score']:.4f}\"\n",
    "            report += f\"\\n  - Support: {scores['support']}\"\n",
    "    \n",
    "    # ì˜¤ë¶„ë¥˜ ë¶„ì„ ì¶”ê°€\n",
    "    report += \"\\n\\n=== Error Analysis ===\\n\"\n",
    "    error_matrix = {}\n",
    "    for true, pred in zip(true_labels, predictions):\n",
    "        if true != pred:\n",
    "            key = f\"{true} â†’ {pred}\"\n",
    "            error_matrix[key] = error_matrix.get(key, 0) + 1\n",
    "    \n",
    "    report += \"\\nMost common misclassifications:\\n\"\n",
    "    for error, count in sorted(error_matrix.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        report += f\"  {error}: {count} times\\n\"\n",
    "    \n",
    "    with open(f\"{output_dir}/test_report.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    logger.info(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92421a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë””ë°”ì´ìŠ¤ ê°•ì œ ì´ë™ í•¨ìˆ˜\n",
    "def force_move_all(model, device):\n",
    "    model.to(device)\n",
    "    for name, module in model.named_children():\n",
    "        try:\n",
    "            module.to(device)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# âœ… ë””ë°”ì´ìŠ¤ í™•ì¸ìš© (ë””ë²„ê¹…ìš©ì´ë¯€ë¡œ ì¶”í›„ ì œê±°í•´ë„ ë¨)\n",
    "def print_model_devices(model, device):\n",
    "    print(\"ğŸ“¦ Model ì„œë¸Œëª¨ë“ˆ ë””ë°”ì´ìŠ¤ í™•ì¸:\")\n",
    "    for name, module in model.named_modules():\n",
    "        try:\n",
    "            param = next(module.parameters())\n",
    "            if param.device != torch.device(device):\n",
    "                print(f\"âŒ {name}: {param.device} (Expected: {device})\")\n",
    "            else:\n",
    "                print(f\"âœ… {name}: {param.device}\")\n",
    "        except StopIteration:\n",
    "            print(f\"âš ï¸  {name}: íŒŒë¼ë¯¸í„° ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    # ì„¤ì •\n",
    "    model_path = \"/home/jieun/workspace/clovax/finetune/output/finetuned_hcx_sft_masking\"  # í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ\n",
    "    test_data_path = \"/home/jieun/workspace/clovax/finetune/dataset/test_data/test_dataset.jsonl\"  # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ\n",
    "    output_dir = \"./test_results\"\n",
    "    device = \"cuda:7\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ\n",
    "    model, processor, tokenizer = load_finetuned_model(model_path, device)\n",
    "    \n",
    "    print(\"ğŸš€ ëª¨ë¸ ë””ë°”ì´ìŠ¤:\", next(model.parameters()).device)\n",
    "    \n",
    "    # print_model_devices(model, device)\n",
    "    # ğŸ’ª ê°•ì œë¡œ ëª¨ë‘ ì´ë™\n",
    "    force_move_all(model, device)\n",
    "\n",
    "    # ğŸ” ì´ë™ í›„ í™•ì¸\n",
    "    print(\"âœ… ì´ë™ í›„ ìƒíƒœ:\")\n",
    "    # print_model_devices(model, device)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\" - {name}: {param.device}\")\n",
    "        break\n",
    "    \n",
    "    force_move_all(model, device)\n",
    "    print_model_devices(model, device)\n",
    "    \n",
    "    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "    predictions, true_labels = test_model(\n",
    "        model, processor, tokenizer, test_data_path, output_dir, model_path, device\n",
    "    )\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    metrics = calculate_metrics(true_labels, predictions)\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬ ê·¸ë¦¬ê¸°\n",
    "    cm = plot_confusion_matrix(\n",
    "        true_labels, \n",
    "        predictions, \n",
    "        save_path=f\"{output_dir}/confusion_matrix.png\"\n",
    "    )\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    save_results(predictions, true_labels, metrics, output_dir)\n",
    "    \n",
    "    # ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n=== Per-Class Performance ===\")\n",
    "    emotions = ['í–‰ë³µ', 'ìŠ¬í””', 'ë¶„ë…¸', 'ê³µí¬', 'ë†€ëŒ', 'ì¤‘ë¦½']\n",
    "    for emotion in emotions:\n",
    "        if emotion in metrics['class_report']:\n",
    "            scores = metrics['class_report'][emotion]\n",
    "            print(f\"{emotion}: P={scores['precision']:.3f}, R={scores['recall']:.3f}, F1={scores['f1-score']:.3f}, Support={scores['support']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7b165",
   "metadata": {},
   "source": [
    "### ê° ë¼ë²¨ ë³„ ì˜ˆì¸¡ ë¹ˆë„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21503638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_label  unknown  ê³µí¬   ë†€ëŒ   ë¶„ë…¸   ìŠ¬í””   ì¤‘ë¦½   í–‰ë³µ\n",
      "true_label                                           \n",
      "ê³µí¬                     4  55    8    5  115  202   11\n",
      "ë†€ëŒ                    14   4  105   16    3  233   25\n",
      "ë¶„ë…¸                     6   8    3  187    7  176   13\n",
      "ìŠ¬í””                     2   4   14   32  185  151   12\n",
      "ì¤‘ë¦½                    18   1    7   15    6  323   30\n",
      "í–‰ë³µ                     1   0    7    7    8   94  283\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mí˜„ì¬ ì…€ ë˜ëŠ” ì´ì „ ì…€ì—ì„œ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ Kernelì´ ì¶©ëŒí–ˆìŠµë‹ˆë‹¤. \n",
      "\u001b[1;31mì…€ì˜ ì½”ë“œë¥¼ ê²€í† í•˜ì—¬ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ì›ì¸ì„ ì‹ë³„í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì„ ë³´ë ¤ë©´ <a href='https://aka.ms/vscodeJupyterKernelCrash'>ì—¬ê¸°</a>ë¥¼ í´ë¦­í•˜ì„¸ìš”. \n",
      "\u001b[1;31mìì„¸í•œ ë‚´ìš©ì€ Jupyter <a href='command:jupyter.viewOutput'>ë¡œê·¸</a>ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
     ]
    }
   ],
   "source": [
    "# predict.csv íŒŒì¼ ë¡œë“œ\n",
    "df = pd.read_csv(\"/home/jieun/workspace/clovax/finetune/sft/test_results/predictions.csv\")  # íŒŒì¼ ê²½ë¡œ í•„ìš” ì‹œ ìˆ˜ì •\n",
    "\n",
    "# true_labelë³„ë¡œ predicted_label ì¹´ìš´íŠ¸\n",
    "grouped_counts = df.groupby(\"true_label\")[\"predicted_label\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(grouped_counts)\n",
    "\n",
    "# ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´:\n",
    "grouped_counts.to_csv(\"label_prediction_counts.csv\", encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb8c01b",
   "metadata": {},
   "source": [
    "## ======================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1c652",
   "metadata": {},
   "source": [
    "### SFT Model test code (naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B) / Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af595ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clovax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
